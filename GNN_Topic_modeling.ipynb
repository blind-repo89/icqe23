{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f241cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/GNN_Python39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "from torch_geometric.nn import GCNConv\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch_geometric.utils import from_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "import string\n",
    "import random\n",
    "import torch_geometric\n",
    "import os\n",
    "import time\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a5fd8",
   "metadata": {},
   "source": [
    "# Tokenization, stopword removal, and stemming or lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a123d9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/percyjardine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/percyjardine/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/percyjardine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv('input_data.csv')\n",
    "original_data_with_topics = data.copy()\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Define a list of nonsense words\n",
    "    nonsense_words = [\"oh\", \"umm\", \"oops\", \"hi\", \"lol\", \"rofl\", \"lmao\", \"wtf\", \"omg\", \n",
    "                     \"ok\", \"right\", \"uh\", \"huh\", \"yep\", \"ohh\", \"hmm\", \"ah\", \"god\", \"shit\", \n",
    "                      \"like\", \"say\", \"oop\", \"yeah\", \"yes\", \"xxxx\", \"ca\", \"na\", \"ohhh\", \"yo\", \"wow\", \"whoa\", \n",
    "                      \"shit\", \"sucking\", \"uhh\", \"inaudible\"]\n",
    "\n",
    "    # Remove nonsense words\n",
    "    tokens = [token for token in tokens if token not in nonsense_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Apply the preprocessing function to the \"All_Content\" column\n",
    "data['Processed_Content'] = data['All_Content'].apply(preprocess_text)\n",
    "\n",
    "# filter the rows that have an empty list in col1\n",
    "rows_to_drop = data['Processed_Content'].apply(lambda x: len(x) == 0)\n",
    "dropped_indices = data[rows_to_drop].index\n",
    "\n",
    "# drop the rows that meet the condition\n",
    "data = data[~rows_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebbc40b",
   "metadata": {},
   "source": [
    "**Find word frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80cb58f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2848), ('i', 1287), ('so', 1282), ('it', 1024), ('is', 966), ('that', 928), ('and', 905), ('to', 854), ('wa', 743), ('we', 712), ('a', 622), ('have', 619), ('this', 614), ('you', 554), ('kelley', 516), ('he', 497), ('do', 476), ('at', 471), ('no', 452), ('what', 451), ('one', 446), ('in', 432), ('there', 408), ('but', 406), ('smith', 382), ('scott', 376), ('think', 345), ('of', 341), ('on', 333), ('can', 307), ('apartment', 306), ('jones', 278), ('then', 277), ('just', 271), ('here', 266), ('key', 256), ('who', 251), ('because', 236), ('not', 228), ('from', 213), ('found', 213), ('miss', 211), ('knife', 203), ('ellington', 197), ('time', 195), ('with', 189), ('for', 187), ('had', 180), ('be', 178), ('they', 176), ('margaret', 169), ('know', 167), ('all', 166), ('elwood', 166), ('she', 161), ('about', 160), ('maybe', 157), ('did', 153), ('bank', 152), ('are', 151), ('when', 146), ('or', 145), ('my', 142), ('ha', 141), ('person', 139), ('president', 138), ('well', 137), ('howard', 136), ('went', 135), ('murder', 128), ('shot', 127), ('wait', 127), ('got', 124), ('if', 123), ('where', 123), ('why', 121), ('dynamite', 121), ('wife', 119), ('after', 119), ('albert', 119), ('hippie', 119), ('front', 118), ('thursday', 115), ('his', 113), ('body', 112), ('door', 111), ('me', 110), ('clue', 110), ('something', 109), ('pm', 109), ('said', 106), ('also', 104), ('two', 103), ('mean', 102), ('let', 102), ('see', 100), ('robbery', 100), ('flower', 97), ('should', 92), ('go', 91)]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Concatenate all the strings in the \"Processed_Content\" column into one long string\n",
    "all_content = ' '.join(data['Processed_Content'].explode())\n",
    "\n",
    "# Step 2: Convert the long string to lowercase\n",
    "all_content = all_content.lower()\n",
    "\n",
    "# Step 3: Remove all punctuation and non-alphanumeric characters\n",
    "translator = str.maketrans('', '', string.punctuation + 'â€™')\n",
    "all_content = all_content.translate(translator)\n",
    "\n",
    "# Step 4: Split the long string into individual words\n",
    "words = all_content.split()\n",
    "\n",
    "# Step 5: Count the frequency of each word\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Print the 100 most common words and their frequencies\n",
    "print(word_counts.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f813b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PlayerID</th>\n",
       "      <th>GroupID</th>\n",
       "      <th>Condition</th>\n",
       "      <th>All_Content</th>\n",
       "      <th>information</th>\n",
       "      <th>grounds</th>\n",
       "      <th>claim</th>\n",
       "      <th>organization</th>\n",
       "      <th>query</th>\n",
       "      <th>social</th>\n",
       "      <th>strategy</th>\n",
       "      <th>Processed_Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Alright. Is that OK that we can hear you guys ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[alright, is, that, that, we, can, hear, you, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>This is overwhelming.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, overwhelming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Oh wow. This is. So.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, so]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>So do we.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[so, do, we]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>I think all you could do 'cause I mean this is...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, think, all, you, could, do, i, mean, this,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6175</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Janitor.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[janitor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6176</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Who supplied the weapon?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[who, supplied, the, weapon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6177</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>The first note. You can see the first...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, first, note, you, can, see, the, first]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6179</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Jaguar.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[jaguar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6180</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Jaguar.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[jaguar]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5096 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PlayerID  GroupID Condition  \\\n",
       "0            3        1   Desktop   \n",
       "2            7        1   Desktop   \n",
       "3            9        1   Desktop   \n",
       "4            3        1   Desktop   \n",
       "5            9        1   Desktop   \n",
       "...        ...      ...       ...   \n",
       "6175         2        7   Desktop   \n",
       "6176         5        7   Desktop   \n",
       "6177         2        7   Desktop   \n",
       "6179         4        7   Desktop   \n",
       "6180         3        7   Desktop   \n",
       "\n",
       "                                            All_Content  information  grounds  \\\n",
       "0     Alright. Is that OK that we can hear you guys ...            0        0   \n",
       "2                                 This is overwhelming.            0        0   \n",
       "3                                  Oh wow. This is. So.            0        0   \n",
       "4                                             So do we.            0        0   \n",
       "5     I think all you could do 'cause I mean this is...            0        0   \n",
       "...                                                 ...          ...      ...   \n",
       "6175                                           Janitor.            0        0   \n",
       "6176                           Who supplied the weapon?            0        0   \n",
       "6177           The first note. You can see the first...            0        0   \n",
       "6179                                            Jaguar.            0        0   \n",
       "6180                                            Jaguar.            0        0   \n",
       "\n",
       "      claim  organization  query  social  strategy  \\\n",
       "0         0             0      1       0         0   \n",
       "2         0             0      0       0         0   \n",
       "3         0             0      0       0         0   \n",
       "4         0             0      0       0         0   \n",
       "5         0             1      1       0         1   \n",
       "...     ...           ...    ...     ...       ...   \n",
       "6175      0             0      0       0         0   \n",
       "6176      1             0      1       0         0   \n",
       "6177      0             0      0       0         0   \n",
       "6179      0             0      0       0         0   \n",
       "6180      0             0      0       0         0   \n",
       "\n",
       "                                      Processed_Content  \n",
       "0     [alright, is, that, that, we, can, hear, you, ...  \n",
       "2                              [this, is, overwhelming]  \n",
       "3                                        [this, is, so]  \n",
       "4                                          [so, do, we]  \n",
       "5     [i, think, all, you, could, do, i, mean, this,...  \n",
       "...                                                 ...  \n",
       "6175                                          [janitor]  \n",
       "6176                       [who, supplied, the, weapon]  \n",
       "6177      [the, first, note, you, can, see, the, first]  \n",
       "6179                                           [jaguar]  \n",
       "6180                                           [jaguar]  \n",
       "\n",
       "[5096 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ed973",
   "metadata": {},
   "source": [
    "# Construct a document-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ffcd54",
   "metadata": {},
   "source": [
    "**To calculate the TF-IDF weight for a term in a document (cell (i, j) in the matrix), we need to compute the Term Frequency (TF) and Inverse Document Frequency (IDF) values and then multiply them together. Here's a step-by-step explanation of how to do this:**\n",
    "\n",
    "TF(i, j) = (Number of times term j appears in document i) / (Total number of terms in document i)\n",
    "\n",
    "IDF(j) = log( (Total number of documents) / (Number of documents containing term j) )\n",
    "\n",
    "**Calculate TF-IDF weight for term j in document i, the TF-IDF weight is obtained by multiplying the TF and IDF values:**\n",
    "\n",
    "TF-IDF(i, j) = TF(i, j) * IDF(j)3\n",
    "\n",
    "**The resulting TF-IDF weight captures the term's importance within the individual document and across the entire collection of documents.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae75e8e",
   "metadata": {},
   "source": [
    "**1. n grams tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ebee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2378ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/GNN_Python39/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'Processed_Content' column back to string format\n",
    "data['Processed_Content_String'] = data['Processed_Content'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "#By setting the ngram_range parameter to (1, 2), \n",
    "#the vectorizer will consider both single words (uni-grams) and bi-grams. \n",
    "#If you want to include tri-grams as well, you can set the ngram_range to (1, 3)\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, preprocessor=custom_preprocessor, ngram_range=(2, 3))\n",
    "document_term_matrix = vectorizer.fit_transform(data['Processed_Content_String'])\n",
    "document_term_matrix_df = pd.DataFrame(document_term_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Save the document-term matrix to a CSV file\n",
    "document_term_matrix_df.to_csv('document_term_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b6d81",
   "metadata": {},
   "source": [
    "**2. uni-gram tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a77a3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Processed_Content' column back to string format\n",
    "#data['Processed_Content_String'] = data['Processed_Content'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "#vectorizer = TfidfVectorizer()\n",
    "#document_term_matrix = vectorizer.fit_transform(data['Processed_Content_String'])\n",
    "\n",
    "# Save the document-term matrix and feature names to a dataframe\n",
    "#document_term_matrix_df = pd.DataFrame(document_term_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Save the document-term matrix to a CSV file\n",
    "#document_term_matrix_df.to_csv('document_term_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fb26bc",
   "metadata": {},
   "source": [
    "# Desktop or VR condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1d4db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_factor = 10\n",
    "data['Condition_Scaled'] = data['Condition'].apply(lambda x: scaling_factor if x == 'VR' else 0)\n",
    "document_term_matrix_df['Condition_Scaled'] = data['Condition_Scaled'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6401e88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(document_term_matrix_df['Condition_Scaled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11948b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a a</th>\n",
       "      <th>a a clear</th>\n",
       "      <th>a a concept</th>\n",
       "      <th>a a for</th>\n",
       "      <th>a a grudge</th>\n",
       "      <th>a a hippie</th>\n",
       "      <th>a a kind</th>\n",
       "      <th>a a loose</th>\n",
       "      <th>a agnes</th>\n",
       "      <th>a agnes can</th>\n",
       "      <th>...</th>\n",
       "      <th>yours is</th>\n",
       "      <th>yup he</th>\n",
       "      <th>yup he ha</th>\n",
       "      <th>yuwei albert</th>\n",
       "      <th>yuwei albert greenbag</th>\n",
       "      <th>yuwei shui</th>\n",
       "      <th>yuwei shui anything</th>\n",
       "      <th>zoom please</th>\n",
       "      <th>zoom please no</th>\n",
       "      <th>Condition_Scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5093</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5096 rows Ã— 36962 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      a a  a a clear  a a concept  a a for  a a grudge  a a hippie  a a kind  \\\n",
       "0     0.0        0.0          0.0      0.0         0.0         0.0       0.0   \n",
       "1     0.0        0.0          0.0      0.0         0.0         0.0       0.0   \n",
       "2     0.0        0.0          0.0      0.0         0.0         0.0       0.0   \n",
       "3     0.0        0.0          0.0      0.0         0.0         0.0       0.0   \n",
       "4     0.0        0.0          0.0      0.0         0.0         0.0       0.0   \n",
       "...   ...        ...          ...      ...         ...         ...       ...   \n",
       "5091  0.0        0.0          0.0      0.0         0.0         0.0       0.0   \n",
       "5092  0.0        0.0          0.0      0.0         0.0         0.0       0.0   \n",
       "5093  0.0        0.0          0.0      0.0         0.0         0.0       0.0   \n",
       "5094  0.0        0.0          0.0      0.0         0.0         0.0       0.0   \n",
       "5095  0.0        0.0          0.0      0.0         0.0         0.0       0.0   \n",
       "\n",
       "      a a loose  a agnes  a agnes can  ...  yours is  yup he  yup he ha  \\\n",
       "0           0.0      0.0          0.0  ...       0.0     0.0        0.0   \n",
       "1           0.0      0.0          0.0  ...       0.0     0.0        0.0   \n",
       "2           0.0      0.0          0.0  ...       0.0     0.0        0.0   \n",
       "3           0.0      0.0          0.0  ...       0.0     0.0        0.0   \n",
       "4           0.0      0.0          0.0  ...       0.0     0.0        0.0   \n",
       "...         ...      ...          ...  ...       ...     ...        ...   \n",
       "5091        0.0      0.0          0.0  ...       0.0     0.0        0.0   \n",
       "5092        0.0      0.0          0.0  ...       0.0     0.0        0.0   \n",
       "5093        0.0      0.0          0.0  ...       0.0     0.0        0.0   \n",
       "5094        0.0      0.0          0.0  ...       0.0     0.0        0.0   \n",
       "5095        0.0      0.0          0.0  ...       0.0     0.0        0.0   \n",
       "\n",
       "      yuwei albert  yuwei albert greenbag  yuwei shui  yuwei shui anything  \\\n",
       "0              0.0                    0.0         0.0                  0.0   \n",
       "1              0.0                    0.0         0.0                  0.0   \n",
       "2              0.0                    0.0         0.0                  0.0   \n",
       "3              0.0                    0.0         0.0                  0.0   \n",
       "4              0.0                    0.0         0.0                  0.0   \n",
       "...            ...                    ...         ...                  ...   \n",
       "5091           0.0                    0.0         0.0                  0.0   \n",
       "5092           0.0                    0.0         0.0                  0.0   \n",
       "5093           0.0                    0.0         0.0                  0.0   \n",
       "5094           0.0                    0.0         0.0                  0.0   \n",
       "5095           0.0                    0.0         0.0                  0.0   \n",
       "\n",
       "      zoom please  zoom please no  Condition_Scaled  \n",
       "0             0.0             0.0                 0  \n",
       "1             0.0             0.0                 0  \n",
       "2             0.0             0.0                 0  \n",
       "3             0.0             0.0                 0  \n",
       "4             0.0             0.0                 0  \n",
       "...           ...             ...               ...  \n",
       "5091          0.0             0.0                 0  \n",
       "5092          0.0             0.0                 0  \n",
       "5093          0.0             0.0                 0  \n",
       "5094          0.0             0.0                 0  \n",
       "5095          0.0             0.0                 0  \n",
       "\n",
       "[5096 rows x 36962 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ffd3c",
   "metadata": {},
   "source": [
    "# Compute the document similarity and create a document graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e7935",
   "metadata": {},
   "source": [
    "It is reasonable to calculate the cosine similarity for the TF-IDF matrix. In fact, it is a common technique in text mining and information retrieval to measure the similarity between documents based on their term frequency-inverse document frequency (TF-IDF) vectors.\n",
    "\n",
    "TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. By calculating the cosine similarity between the TF-IDF vectors of documents, we can measure how similar the documents are in terms of their word usage and importance.\n",
    "\n",
    "Cosine similarity is particularly well-suited for this task because it normalizes the vectors before calculating the similarity, making it less sensitive to the length of the documents. This means that even if two documents have different lengths but use similar words with similar importance, their cosine similarity will still be high.\n",
    "\n",
    "By computing the cosine similarity of the TF-IDF matrix and creating a graph based on this similarity, we can capture the relationships between the documents in the dataset and use these relationships for tasks such as clustering, classification, or generating embeddings with graph neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0bf4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame that only contains the text features\n",
    "text_features_df = document_term_matrix_df.drop(columns=[\"Condition_Scaled\"])\n",
    "\n",
    "# Compute document similarity using cosine similarity\n",
    "similarity_matrix = cosine_similarity(text_features_df)\n",
    "\n",
    "# Threshold the similarity matrix to create adjacency matrix\n",
    "threshold = 0.1\n",
    "adjacency_matrix = (similarity_matrix > threshold).astype(int)\n",
    "\n",
    "#document_graph = nx.from_numpy_matrix(adjacency_matrix) for networkx below 3.0\n",
    "document_graph = nx.DiGraph(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2170775",
   "metadata": {},
   "source": [
    "# Train a Graph Convolutional Network (GCN) on the document graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e7beba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_contrastive_loss(out, edge_index, neg_sampling_ratio=5, margin=1.0):\n",
    "    pos_loss = F.mse_loss(out[edge_index[0]], out[edge_index[1]])\n",
    "\n",
    "    num_nodes = out.shape[0]\n",
    "    neg_indices = torch.randint(0, num_nodes, (2, neg_sampling_ratio * edge_index.shape[1]))\n",
    "    neg_loss = F.relu(margin - F.mse_loss(out[neg_indices[0]], out[neg_indices[1]], reduction='none')).mean()\n",
    "\n",
    "    return pos_loss + neg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c11bd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.9384158849716187\n",
      "Epoch: 20, Loss: 0.5793344974517822\n",
      "Epoch: 30, Loss: 0.4834437966346741\n",
      "Epoch: 40, Loss: 0.4169653654098511\n",
      "Epoch: 50, Loss: 0.39129966497421265\n",
      "Epoch: 60, Loss: 0.3799734115600586\n",
      "Epoch: 70, Loss: 0.3714906871318817\n",
      "Epoch: 80, Loss: 0.3686862885951996\n",
      "Epoch: 90, Loss: 0.366213858127594\n",
      "Epoch: 100, Loss: 0.3636719882488251\n",
      "Epoch: 110, Loss: 0.36402755975723267\n",
      "Epoch: 120, Loss: 0.3605881333351135\n",
      "Epoch: 130, Loss: 0.3610188364982605\n",
      "Epoch: 140, Loss: 0.36048227548599243\n",
      "Epoch: 150, Loss: 0.3612399399280548\n",
      "Epoch: 160, Loss: 0.3616284132003784\n",
      "Epoch: 170, Loss: 0.3650686740875244\n",
      "Epoch: 180, Loss: 0.3596373498439789\n",
      "Epoch: 190, Loss: 0.35949674248695374\n",
      "Epoch: 200, Loss: 0.35949498414993286\n",
      "Epoch: 210, Loss: 0.35819873213768005\n",
      "Epoch: 220, Loss: 0.3595665693283081\n",
      "Epoch: 230, Loss: 0.35817408561706543\n",
      "Epoch: 240, Loss: 0.3576695919036865\n",
      "Epoch: 250, Loss: 0.35983380675315857\n",
      "Epoch: 260, Loss: 0.35836243629455566\n",
      "Epoch: 270, Loss: 0.3577459454536438\n",
      "Epoch: 280, Loss: 0.35778194665908813\n",
      "Epoch: 290, Loss: 0.3602229356765747\n",
      "Epoch: 300, Loss: 0.3574984073638916\n",
      "Epoch: 310, Loss: 0.3574489653110504\n",
      "Epoch: 320, Loss: 0.3568074405193329\n",
      "Epoch: 330, Loss: 0.35903796553611755\n",
      "Epoch: 340, Loss: 0.35779711604118347\n",
      "Epoch: 350, Loss: 0.35815173387527466\n",
      "Epoch: 360, Loss: 0.3565918207168579\n",
      "Epoch: 370, Loss: 0.3561738133430481\n",
      "Epoch: 380, Loss: 0.35706454515457153\n",
      "Epoch: 390, Loss: 0.35736507177352905\n",
      "Epoch: 400, Loss: 0.3569420576095581\n",
      "Epoch: 410, Loss: 0.357440322637558\n",
      "Epoch: 420, Loss: 0.3562292456626892\n",
      "Epoch: 430, Loss: 0.3570369482040405\n",
      "Epoch: 440, Loss: 0.35572025179862976\n",
      "Epoch: 450, Loss: 0.35611218214035034\n",
      "Epoch: 460, Loss: 0.35775965452194214\n",
      "Epoch: 470, Loss: 0.3564760088920593\n",
      "Epoch: 480, Loss: 0.3562764823436737\n",
      "Epoch: 490, Loss: 0.35669100284576416\n",
      "Epoch: 500, Loss: 0.35778146982192993\n",
      "Epoch: 510, Loss: 0.3558860421180725\n",
      "Epoch: 520, Loss: 0.35607537627220154\n",
      "Epoch: 530, Loss: 0.3563155233860016\n",
      "Epoch: 540, Loss: 0.35737359523773193\n",
      "Epoch: 550, Loss: 0.3564732074737549\n",
      "Epoch: 560, Loss: 0.35648083686828613\n",
      "Epoch: 570, Loss: 0.35665953159332275\n",
      "Epoch: 580, Loss: 0.3544986844062805\n",
      "Epoch: 590, Loss: 0.3573351800441742\n",
      "Epoch: 600, Loss: 0.355745404958725\n",
      "Epoch: 610, Loss: 0.3553955554962158\n",
      "Epoch: 620, Loss: 0.35613763332366943\n",
      "Epoch: 630, Loss: 0.3559650480747223\n",
      "Epoch: 640, Loss: 0.3557800054550171\n",
      "Epoch: 650, Loss: 0.3549286723136902\n",
      "Epoch: 660, Loss: 0.35514435172080994\n",
      "Epoch: 670, Loss: 0.35459762811660767\n",
      "Epoch: 680, Loss: 0.35622739791870117\n",
      "Epoch: 690, Loss: 0.3551994562149048\n",
      "Epoch: 700, Loss: 0.3554036617279053\n",
      "Epoch: 710, Loss: 0.3555230498313904\n",
      "Epoch: 720, Loss: 0.355793297290802\n",
      "Epoch: 730, Loss: 0.3548196852207184\n",
      "Epoch: 740, Loss: 0.3554084002971649\n",
      "Epoch: 750, Loss: 0.35652437806129456\n",
      "Epoch: 760, Loss: 0.3544260263442993\n",
      "Epoch: 770, Loss: 0.35536664724349976\n",
      "Epoch: 780, Loss: 0.35513654351234436\n",
      "Epoch: 790, Loss: 0.35602179169654846\n",
      "Epoch: 800, Loss: 0.3544250726699829\n",
      "Epoch: 810, Loss: 0.3557875156402588\n",
      "Epoch: 820, Loss: 0.3551914095878601\n",
      "Epoch: 830, Loss: 0.35504427552223206\n",
      "Epoch: 840, Loss: 0.3555546998977661\n",
      "Epoch: 850, Loss: 0.3556694984436035\n",
      "Epoch: 860, Loss: 0.3552916347980499\n",
      "Epoch: 870, Loss: 0.3542369604110718\n",
      "Epoch: 880, Loss: 0.35535338521003723\n",
      "Epoch: 890, Loss: 0.35541415214538574\n",
      "Epoch: 900, Loss: 0.35497143864631653\n",
      "Epoch: 910, Loss: 0.3547673523426056\n",
      "Epoch: 920, Loss: 0.35485899448394775\n",
      "Epoch: 930, Loss: 0.35499534010887146\n",
      "Epoch: 940, Loss: 0.35385769605636597\n",
      "Epoch: 950, Loss: 0.35446542501449585\n",
      "Epoch: 960, Loss: 0.35552477836608887\n",
      "Epoch: 970, Loss: 0.35531705617904663\n",
      "Epoch: 980, Loss: 0.3576398491859436\n",
      "Epoch: 990, Loss: 0.35550203919410706\n",
      "Epoch: 1000, Loss: 0.35620835423469543\n",
      "Execution time: 365.5487401485443 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Add the set_seed function\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#The purpose of using both GCN and GAT is to leverage the strengths of both methods: \n",
    "#GCN for local graph structure and \n",
    "#GAT for capturing more global structure through attention mechanisms.\n",
    "class ComplexTopicGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super(ComplexTopicGNN, self).__init__()\n",
    "        self.gcn1 = GCNConv(num_features, hidden_channels)\n",
    "        self.gcn2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.gat1 = GATConv(hidden_channels, hidden_channels, heads=2)\n",
    "        self.gat2 = GATConv(hidden_channels * 2, hidden_channels, heads=1)\n",
    "        self.lin1 = torch.nn.Linear(1, 32)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels + 32, 32)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x[:, :-1], data.edge_index\n",
    "        condition = data.x[:, -1].view(-1, 1)\n",
    "        \n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        condition = self.lin1(condition)\n",
    "        condition = F.relu(condition)\n",
    "\n",
    "        x = torch.cat((x, condition), dim=1)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Set the seed value\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "torch_geometric.seed_everything(seed)\n",
    "\n",
    "# Convert the document graph to PyTorch Geometric format\n",
    "pyg_data = from_networkx(document_graph)\n",
    "pyg_data.x = torch.tensor(document_term_matrix_df.to_numpy(), dtype=torch.float)\n",
    "\n",
    "# Initialize and train the GCN model\n",
    "num_features = document_term_matrix_df.shape[1] - 1\n",
    "hidden_channels = 32\n",
    "model = ComplexTopicGNN(num_features, hidden_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 1000\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(pyg_data)\n",
    "    loss = graph_contrastive_loss(out, pyg_data.edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "document_embeddings = out.detach().numpy()\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "print(f\"Execution time: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f19621d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.405627</td>\n",
       "      <td>-0.742578</td>\n",
       "      <td>-0.557294</td>\n",
       "      <td>0.452803</td>\n",
       "      <td>-0.403072</td>\n",
       "      <td>0.444927</td>\n",
       "      <td>-0.460850</td>\n",
       "      <td>0.656498</td>\n",
       "      <td>0.883746</td>\n",
       "      <td>-0.286310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.559714</td>\n",
       "      <td>-0.741436</td>\n",
       "      <td>-0.546874</td>\n",
       "      <td>0.365763</td>\n",
       "      <td>-0.668594</td>\n",
       "      <td>0.632117</td>\n",
       "      <td>-0.454748</td>\n",
       "      <td>0.553281</td>\n",
       "      <td>0.182276</td>\n",
       "      <td>0.523110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.551400</td>\n",
       "      <td>1.180257</td>\n",
       "      <td>1.398181</td>\n",
       "      <td>-1.498776</td>\n",
       "      <td>1.541236</td>\n",
       "      <td>-1.498615</td>\n",
       "      <td>1.490072</td>\n",
       "      <td>-1.300453</td>\n",
       "      <td>-1.044214</td>\n",
       "      <td>1.659183</td>\n",
       "      <td>...</td>\n",
       "      <td>1.401595</td>\n",
       "      <td>1.197007</td>\n",
       "      <td>1.374483</td>\n",
       "      <td>-1.605782</td>\n",
       "      <td>1.254378</td>\n",
       "      <td>-1.310816</td>\n",
       "      <td>1.499511</td>\n",
       "      <td>-1.371670</td>\n",
       "      <td>-1.765511</td>\n",
       "      <td>-1.434016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.556977</td>\n",
       "      <td>1.185761</td>\n",
       "      <td>1.401792</td>\n",
       "      <td>-1.504025</td>\n",
       "      <td>1.546108</td>\n",
       "      <td>-1.503773</td>\n",
       "      <td>1.498381</td>\n",
       "      <td>-1.307976</td>\n",
       "      <td>-1.052049</td>\n",
       "      <td>1.670102</td>\n",
       "      <td>...</td>\n",
       "      <td>1.411865</td>\n",
       "      <td>1.201715</td>\n",
       "      <td>1.385018</td>\n",
       "      <td>-1.613614</td>\n",
       "      <td>1.262675</td>\n",
       "      <td>-1.316286</td>\n",
       "      <td>1.505426</td>\n",
       "      <td>-1.378521</td>\n",
       "      <td>-1.773341</td>\n",
       "      <td>-1.440802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.926496</td>\n",
       "      <td>0.550716</td>\n",
       "      <td>0.763938</td>\n",
       "      <td>-0.873119</td>\n",
       "      <td>0.914447</td>\n",
       "      <td>-0.873556</td>\n",
       "      <td>0.872850</td>\n",
       "      <td>-0.680091</td>\n",
       "      <td>-0.426285</td>\n",
       "      <td>1.036576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792013</td>\n",
       "      <td>0.571362</td>\n",
       "      <td>0.765484</td>\n",
       "      <td>-0.985919</td>\n",
       "      <td>0.638078</td>\n",
       "      <td>-0.690404</td>\n",
       "      <td>0.874788</td>\n",
       "      <td>-0.748800</td>\n",
       "      <td>-1.137492</td>\n",
       "      <td>-0.811212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.467464</td>\n",
       "      <td>0.094065</td>\n",
       "      <td>0.301118</td>\n",
       "      <td>-0.407293</td>\n",
       "      <td>0.455923</td>\n",
       "      <td>-0.415230</td>\n",
       "      <td>0.413753</td>\n",
       "      <td>-0.211734</td>\n",
       "      <td>0.034085</td>\n",
       "      <td>0.581195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319913</td>\n",
       "      <td>0.111130</td>\n",
       "      <td>0.304105</td>\n",
       "      <td>-0.524146</td>\n",
       "      <td>0.180577</td>\n",
       "      <td>-0.225671</td>\n",
       "      <td>0.412049</td>\n",
       "      <td>-0.296182</td>\n",
       "      <td>-0.676247</td>\n",
       "      <td>-0.349554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>160.744278</td>\n",
       "      <td>-158.828491</td>\n",
       "      <td>-158.155151</td>\n",
       "      <td>164.662231</td>\n",
       "      <td>-156.939972</td>\n",
       "      <td>163.852829</td>\n",
       "      <td>-160.626160</td>\n",
       "      <td>158.123398</td>\n",
       "      <td>166.157516</td>\n",
       "      <td>-154.670151</td>\n",
       "      <td>...</td>\n",
       "      <td>-160.017731</td>\n",
       "      <td>-160.766174</td>\n",
       "      <td>-160.700165</td>\n",
       "      <td>163.499863</td>\n",
       "      <td>-159.851578</td>\n",
       "      <td>163.155014</td>\n",
       "      <td>-159.389664</td>\n",
       "      <td>165.553452</td>\n",
       "      <td>158.373245</td>\n",
       "      <td>163.565216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>-1.587602</td>\n",
       "      <td>1.216832</td>\n",
       "      <td>1.429350</td>\n",
       "      <td>-1.533701</td>\n",
       "      <td>1.586113</td>\n",
       "      <td>-1.535380</td>\n",
       "      <td>1.538576</td>\n",
       "      <td>-1.338516</td>\n",
       "      <td>-1.083363</td>\n",
       "      <td>1.693159</td>\n",
       "      <td>...</td>\n",
       "      <td>1.445041</td>\n",
       "      <td>1.229995</td>\n",
       "      <td>1.420104</td>\n",
       "      <td>-1.648054</td>\n",
       "      <td>1.294230</td>\n",
       "      <td>-1.350342</td>\n",
       "      <td>1.537922</td>\n",
       "      <td>-1.415547</td>\n",
       "      <td>-1.804613</td>\n",
       "      <td>-1.467561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5093</th>\n",
       "      <td>-1.137762</td>\n",
       "      <td>0.769868</td>\n",
       "      <td>0.975001</td>\n",
       "      <td>-1.078286</td>\n",
       "      <td>1.135165</td>\n",
       "      <td>-1.090626</td>\n",
       "      <td>1.081932</td>\n",
       "      <td>-0.888241</td>\n",
       "      <td>-0.632687</td>\n",
       "      <td>1.249537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990389</td>\n",
       "      <td>0.783504</td>\n",
       "      <td>0.960751</td>\n",
       "      <td>-1.192420</td>\n",
       "      <td>0.853093</td>\n",
       "      <td>-0.900235</td>\n",
       "      <td>1.083566</td>\n",
       "      <td>-0.962804</td>\n",
       "      <td>-1.348858</td>\n",
       "      <td>-1.019791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094</th>\n",
       "      <td>63.806870</td>\n",
       "      <td>-63.039883</td>\n",
       "      <td>-63.083248</td>\n",
       "      <td>65.041130</td>\n",
       "      <td>-63.840302</td>\n",
       "      <td>65.188713</td>\n",
       "      <td>-63.135902</td>\n",
       "      <td>64.691444</td>\n",
       "      <td>64.989906</td>\n",
       "      <td>-60.648033</td>\n",
       "      <td>...</td>\n",
       "      <td>-64.074181</td>\n",
       "      <td>-65.794075</td>\n",
       "      <td>-62.474041</td>\n",
       "      <td>64.010307</td>\n",
       "      <td>-63.755432</td>\n",
       "      <td>66.183952</td>\n",
       "      <td>-63.627411</td>\n",
       "      <td>64.466545</td>\n",
       "      <td>63.253677</td>\n",
       "      <td>65.532646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>191.369949</td>\n",
       "      <td>-184.368729</td>\n",
       "      <td>-188.421188</td>\n",
       "      <td>191.849960</td>\n",
       "      <td>-188.459167</td>\n",
       "      <td>193.989105</td>\n",
       "      <td>-187.955002</td>\n",
       "      <td>191.367905</td>\n",
       "      <td>194.013123</td>\n",
       "      <td>-187.325516</td>\n",
       "      <td>...</td>\n",
       "      <td>-188.400024</td>\n",
       "      <td>-196.156754</td>\n",
       "      <td>-186.428146</td>\n",
       "      <td>189.133743</td>\n",
       "      <td>-190.585373</td>\n",
       "      <td>191.813370</td>\n",
       "      <td>-187.600159</td>\n",
       "      <td>192.628189</td>\n",
       "      <td>190.742203</td>\n",
       "      <td>196.350021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5096 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0       0.405627   -0.742578   -0.557294    0.452803   -0.403072    0.444927   \n",
       "1      -1.551400    1.180257    1.398181   -1.498776    1.541236   -1.498615   \n",
       "2      -1.556977    1.185761    1.401792   -1.504025    1.546108   -1.503773   \n",
       "3      -0.926496    0.550716    0.763938   -0.873119    0.914447   -0.873556   \n",
       "4      -0.467464    0.094065    0.301118   -0.407293    0.455923   -0.415230   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "5091  160.744278 -158.828491 -158.155151  164.662231 -156.939972  163.852829   \n",
       "5092   -1.587602    1.216832    1.429350   -1.533701    1.586113   -1.535380   \n",
       "5093   -1.137762    0.769868    0.975001   -1.078286    1.135165   -1.090626   \n",
       "5094   63.806870  -63.039883  -63.083248   65.041130  -63.840302   65.188713   \n",
       "5095  191.369949 -184.368729 -188.421188  191.849960 -188.459167  193.989105   \n",
       "\n",
       "              6           7           8           9   ...          22  \\\n",
       "0      -0.460850    0.656498    0.883746   -0.286310  ...   -0.559714   \n",
       "1       1.490072   -1.300453   -1.044214    1.659183  ...    1.401595   \n",
       "2       1.498381   -1.307976   -1.052049    1.670102  ...    1.411865   \n",
       "3       0.872850   -0.680091   -0.426285    1.036576  ...    0.792013   \n",
       "4       0.413753   -0.211734    0.034085    0.581195  ...    0.319913   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "5091 -160.626160  158.123398  166.157516 -154.670151  ... -160.017731   \n",
       "5092    1.538576   -1.338516   -1.083363    1.693159  ...    1.445041   \n",
       "5093    1.081932   -0.888241   -0.632687    1.249537  ...    0.990389   \n",
       "5094  -63.135902   64.691444   64.989906  -60.648033  ...  -64.074181   \n",
       "5095 -187.955002  191.367905  194.013123 -187.325516  ... -188.400024   \n",
       "\n",
       "              23          24          25          26          27          28  \\\n",
       "0      -0.741436   -0.546874    0.365763   -0.668594    0.632117   -0.454748   \n",
       "1       1.197007    1.374483   -1.605782    1.254378   -1.310816    1.499511   \n",
       "2       1.201715    1.385018   -1.613614    1.262675   -1.316286    1.505426   \n",
       "3       0.571362    0.765484   -0.985919    0.638078   -0.690404    0.874788   \n",
       "4       0.111130    0.304105   -0.524146    0.180577   -0.225671    0.412049   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "5091 -160.766174 -160.700165  163.499863 -159.851578  163.155014 -159.389664   \n",
       "5092    1.229995    1.420104   -1.648054    1.294230   -1.350342    1.537922   \n",
       "5093    0.783504    0.960751   -1.192420    0.853093   -0.900235    1.083566   \n",
       "5094  -65.794075  -62.474041   64.010307  -63.755432   66.183952  -63.627411   \n",
       "5095 -196.156754 -186.428146  189.133743 -190.585373  191.813370 -187.600159   \n",
       "\n",
       "              29          30          31  \n",
       "0       0.553281    0.182276    0.523110  \n",
       "1      -1.371670   -1.765511   -1.434016  \n",
       "2      -1.378521   -1.773341   -1.440802  \n",
       "3      -0.748800   -1.137492   -0.811212  \n",
       "4      -0.296182   -0.676247   -0.349554  \n",
       "...          ...         ...         ...  \n",
       "5091  165.553452  158.373245  163.565216  \n",
       "5092   -1.415547   -1.804613   -1.467561  \n",
       "5093   -0.962804   -1.348858   -1.019791  \n",
       "5094   64.466545   63.253677   65.532646  \n",
       "5095  192.628189  190.742203  196.350021  \n",
       "\n",
       "[5096 rows x 32 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96a1f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'model_bi_trigram_with_stopword.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91dd6e7",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cddfd9",
   "metadata": {},
   "source": [
    "**Using the ComplexTopicGNN class to learn the document embeddings and then applying a clustering algorithm to get the topics has several advantages:**\n",
    "\n",
    "Exploitation of graph structure: By converting the document-term matrix into a graph and using a graph neural network, you can take advantage of the graph structure present in the data. This can help reveal hidden relationships between documents that are not captured by traditional topic modeling methods like LDA or NMF.\n",
    "\n",
    "Combining GCN and GAT: The ComplexTopicGNN class combines Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT). GCN excels at capturing local graph structure, while GAT can capture more global structure through attention mechanisms. By combining both methods, the model can capture various aspects of the graph structure and learn more informative document embeddings.\n",
    "\n",
    "Flexibility in clustering algorithms: After learning the document embeddings, you can choose from various clustering algorithms (e.g., KMeans, DBSCAN, hierarchical clustering) to suit the specific characteristics of your dataset. This flexibility allows you to experiment with different clustering methods and select the one that best captures the topics in your data.\n",
    "\n",
    "Interpretability: By using a clustering algorithm to group the document embeddings, you can identify the top terms associated with each cluster (topic). These top terms provide an interpretable summary of each topic and facilitate understanding of the themes present in the dataset.\n",
    "\n",
    "Adaptability: The ComplexTopicGNN model can be easily adapted to incorporate additional information or features in the document graph (e.g., metadata, document similarity measures, or node attributes). This adaptability allows you to tailor the model to the specific needs of your dataset and problem domain.\n",
    "\n",
    "Scalability: Graph neural networks can be efficiently parallelized on GPUs, enabling scalability to large datasets. This makes the ComplexTopicGNN model suitable for processing large-scale document collections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebccc0",
   "metadata": {},
   "source": [
    "**Graph neural networks (GNNs) and Latent Dirichlet Allocation (LDA) are different approaches to extracting information from document collections. GNNs can capture relationships that LDA might miss due to their unique way of processing and representing data.**\n",
    "\n",
    "Document relationships: GNNs operate on graph structures, making them well-suited for capturing relationships between documents. These relationships can be based on various factors, such as document similarity. LDA, on the other hand, does not explicitly model relationships between documents, focusing only on the distribution of topics in each document.\n",
    "\n",
    "Heterogeneous information: GNNs can easily incorporate additional information in the form of node and edge attributes, which allows for richer representations and the ability to capture relationships based on diverse types of information. In contrast, LDA only considers the words in a document and their associated probabilities.\n",
    "\n",
    "Higher-order dependencies: GNNs can capture higher-order dependencies between documents, as they iteratively aggregate information from neighboring nodes in the graph. This ability allows GNNs to capture global patterns in the data. LDA is a generative model that assumes independence between documents, limiting its ability to capture higher-order dependencies.\n",
    "\n",
    "Non-linear relationships: GNNs use non-linear activation functions, which enable them to capture complex, non-linear relationships between documents. LDA is a linear generative model, which might miss certain non-linear relationships in the data.\n",
    "\n",
    "End-to-end learning: GNNs learn document embeddings in an end-to-end fashion, optimizing the embeddings based on the overall objective function (e.g., graph contrastive loss). This approach can lead to more meaningful embeddings that capture the desired relationships. LDA, on the other hand, estimates topic distributions using a generative process, which may not result in embeddings that are as tailored to the specific relationships of interest.\n",
    "\n",
    "While GNNs can capture relationships that LDA might miss, it's essential to note that GNNs require a suitable graph representation of the document collection to work effectively. Constructing a meaningful graph representation of the documents is a critical step that influences the performance of the GNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df021e",
   "metadata": {},
   "source": [
    "**Load GNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fb1bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexTopicGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super(ComplexTopicGNN, self).__init__()\n",
    "        self.gcn1 = GCNConv(num_features, hidden_channels)\n",
    "        self.gcn2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.gat1 = GATConv(hidden_channels, hidden_channels, heads=2)\n",
    "        self.gat2 = GATConv(hidden_channels * 2, hidden_channels, heads=1)\n",
    "        self.lin1 = torch.nn.Linear(1, 32)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels + 32, 32)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x[:, :-1], data.edge_index\n",
    "        condition = data.x[:, -1].view(-1, 1)\n",
    "        \n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        condition = self.lin1(condition)\n",
    "        condition = F.relu(condition)\n",
    "\n",
    "        x = torch.cat((x, condition), dim=1)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the document graph to PyTorch Geometric format\n",
    "pyg_data = from_networkx(document_graph)\n",
    "pyg_data.x = torch.tensor(document_term_matrix_df.to_numpy(), dtype=torch.float)\n",
    "\n",
    "# Initialize and train the GCN model\n",
    "num_features = document_term_matrix_df.shape[1] - 1\n",
    "hidden_channels = 32\n",
    "model_loaded = ComplexTopicGNN(num_features, hidden_channels)\n",
    "# load the model back into memory\n",
    "model_loaded.load_state_dict(torch.load('model_bi_trigram_with_stopword.pth'))\n",
    "# Set the model to evaluation mode\n",
    "model_loaded.eval()\n",
    "# Compute the document embeddings\n",
    "out = model_loaded(pyg_data)\n",
    "document_embeddings = out.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e49f8346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.3232720e-01, -4.8410141e-01, -3.0077836e-01, ...,\n",
       "         2.9833424e-01, -7.0594363e-02,  2.5511205e-01],\n",
       "       [-1.6437614e+00,  1.2717584e+00,  1.4814705e+00, ...,\n",
       "        -1.4708718e+00, -1.8608699e+00, -1.5267580e+00],\n",
       "       [-1.6212354e+00,  1.2492424e+00,  1.4586468e+00, ...,\n",
       "        -1.4478384e+00, -1.8377434e+00, -1.5046667e+00],\n",
       "       ...,\n",
       "       [-1.1730410e+00,  8.0557883e-01,  1.0111153e+00, ...,\n",
       "        -9.9792367e-01, -1.3840590e+00, -1.0571390e+00],\n",
       "       [ 9.7656242e+01, -9.6075500e+01, -9.6183914e+01, ...,\n",
       "         1.0018219e+02,  9.8030045e+01,  1.0021991e+02],\n",
       "       [ 9.7656242e+01, -9.6075500e+01, -9.6183914e+01, ...,\n",
       "         1.0018219e+02,  9.8030045e+01,  1.0021991e+02]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babcd234",
   "metadata": {},
   "source": [
    "# 1. Clusted by KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a1b3f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/GNN_Python39/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: i think, miss smith, i have, elwood smith, the key, this one, to the, i do, is the, this is\n",
      "Topic 1: crime analyst, it am, no hello, the unit, uhm so, i gone, so apartment, appeared once, sorry teresa, wordsmith and\n",
      "Topic 2: he inaudbile, look there, eleven pm, when sorry, i bullet, and paper, there wa nothing, wa nothing, just keep together, keep together\n",
      "Topic 3: scott scott, wait i, shui yuwei, bird watch, seeing examiner, smith wait, margarita please, another extension, who jone, jones outside\n",
      "Topic 4: it went, asana please, without making, remove program, something made, another connection, kelley hmmm, or should, look fine, hippie definitely\n",
      "Topic 5: spritz is, change size up, up your panel, up your, spritz is to, inform same, your panel, same size, this spritz is, this spritz\n",
      "Topic 6: cut nigel, this card, make this card, you take margaret, you take, cheating business there, name silent, silent anything, greenberg green greenberg, with blood\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 7\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(document_embeddings)\n",
    "\n",
    "num_top_terms = 10\n",
    "top_terms = {}\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    # Get the indices of the documents in the current cluster\n",
    "    doc_indices = np.where(cluster_labels == cluster)[0]\n",
    "    \n",
    "    # Compute the centroid of the document-term vectors in the current cluster\n",
    "    centroid = np.mean(document_term_matrix_df.iloc[doc_indices, :-1], axis=0)\n",
    "    \n",
    "    # Get the indices of the top terms for the current cluster\n",
    "    top_term_indices = centroid.argsort()[-num_top_terms:][::-1]\n",
    "    \n",
    "    # Get the top terms for the current cluster\n",
    "    top_terms[cluster] = [document_term_matrix_df.columns[idx] for idx in top_term_indices]\n",
    "\n",
    "for cluster, terms in top_terms.items():\n",
    "    print(f\"Topic {cluster}: {', '.join(terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8021a6b",
   "metadata": {},
   "source": [
    "# 2. Clusted by hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f51e06a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: connect sentence, may may, sound good, charge charge, think six, think drag, one question, got nothing, grab panel, get away\n",
      "Topic 2: miss smith, elwood smith, scott apartment, front door, howard ellington, margaret ellington, kelley wife, body found, albert greenbags, key front\n",
      "Topic 3: sorry hear, nice knife, one guy, wait look, keep going, lying lying, lying business, okay okay, explain rationale, thinking maybe\n",
      "Topic 4: teresa green, albert greenback, asana please, bank highlighted, easy delete, feel dizzy, think someone, something happened, guy president, sure sure\n",
      "Topic 5: wife wife, sorry president, lot thing, finger finger, note find, think thursday, closer ohio, duplicated one, role police, aldric disappeared\n",
      "Topic 6: four five, making sure, focus four, focus four five, time scott, left apartment, sure wait, making sure wait, wait overwhelming, sure wait overwhelming\n",
      "Topic 7: crime analyst, name already, give second, created one, kelley minute, sachini created one, sachini created, start solving, sentence sentence, red line\n"
     ]
    }
   ],
   "source": [
    "# Perform hierarchical clustering on the document embeddings\n",
    "linkage_matrix = linkage(document_embeddings, method='ward')\n",
    "\n",
    "# Set the number of clusters (topics)\n",
    "num_clusters = 7\n",
    "\n",
    "# Assign the cluster labels to the documents\n",
    "cluster_labels = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Extract the top terms for each cluster (topic)\n",
    "num_top_terms = 10\n",
    "top_terms = {}\n",
    "\n",
    "for cluster in range(1, num_clusters + 1):  # Cluster labels start from 1\n",
    "    # Get the indices of the documents in the current cluster\n",
    "    doc_indices = np.where(cluster_labels == cluster)[0]\n",
    "\n",
    "    # Compute the centroid of the document-term vectors in the current cluster\n",
    "    centroid = np.mean(document_term_matrix_df.iloc[doc_indices, :-1], axis=0)\n",
    "\n",
    "    # Get the indices of the top terms for the current cluster\n",
    "    top_term_indices = centroid.argsort()[-num_top_terms:][::-1]\n",
    "\n",
    "    # Get the top terms for the current cluster\n",
    "    top_terms[cluster] = [document_term_matrix_df.columns[idx] for idx in top_term_indices]\n",
    "\n",
    "# Print the top terms for each cluster (topic)\n",
    "for cluster, terms in top_terms.items():\n",
    "    print(f\"Topic {cluster}: {', '.join(terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c075f7df",
   "metadata": {},
   "source": [
    "# 3. Baseline model by Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a2e3e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: wednesday afternoon, murder take, construction company, take place, murder take place, knife wound, scott kelley, five pm, hung around, construction company wednesday\n",
      "Topic 1: front door, key front, key front door, howard ellington, body found, jones apartment, key vault, construction company, anything else, opened key\n",
      "Topic 2: miss smith, often followed, albert greenbags president, greenbags president, pm thursday, lobby apartment, albert greenbags, miss miss, let check, smith lobby\n",
      "Topic 3: elwood smith, margaret ellington, kelley wife, one hour, discovered robbery, elwood smith janitor, smith janitor, mexico city, dead one, dead one hour\n",
      "Topic 4: albert greenbags, elwood smith, found park, make sense, arrival time, dog walk, jones shot, body found park, shot intruder, body found\n",
      "Topic 5: scott apartment, went scott, kelley went, went scott apartment, kelley went scott, kelley blood, smith yard, wife went, miss smith, found miss\n",
      "Topic 6: thursday night, dirsey flower, left building, wait wait, time time, frequently left, friday morning, frequently left building, supplied key, thursday night friday\n",
      "Execution time: 11.986347913742065 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_topics = 7\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_model.fit(document_term_matrix_df.iloc[:, :-1])\n",
    "\n",
    "num_top_terms = 10\n",
    "top_terms = {}\n",
    "\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_term_indices = topic.argsort()[-num_top_terms:][::-1]\n",
    "    top_terms[topic_idx] = [document_term_matrix_df.columns[idx] for idx in top_term_indices]\n",
    "\n",
    "for topic, terms in top_terms.items():\n",
    "    print(f\"Topic {topic}: {', '.join(terms)}\")\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "print(f\"Execution time: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "90b5cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign topic labels to each document based on highest topic probability\n",
    "doc_topic_probs = lda_model.transform(document_term_matrix_df.iloc[:, :-1])\n",
    "doc_topic_labels = doc_topic_probs.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b99b0d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4521"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543756c",
   "metadata": {},
   "source": [
    "# Coding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c411182",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Topic'] = kmeans.labels_\n",
    "for topic_num in range(7):\n",
    "    original_data_with_topics[f'Topic_{topic_num}'] = 0\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    topic = row['Topic']\n",
    "    original_data_with_topics.at[index, f'Topic_{topic}'] = 1\n",
    "\n",
    "original_data_with_topics.drop(['information', 'grounds', 'claim',\n",
    "                                'organization', 'query', 'social',\n",
    "                                'strategy'], axis=1).to_csv('coded_dataset_kmeans.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0cd7ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Topic'] = cluster_labels\n",
    "for topic_num in range(7):\n",
    "    original_data_with_topics[f'Topic_{topic_num}'] = 0\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    topic = row['Topic']\n",
    "    original_data_with_topics.at[index, f'Topic_{topic}'] = 1\n",
    "\n",
    "original_data_with_topics.drop(['information', 'grounds', 'claim',\n",
    "                                'organization', 'query', 'social',\n",
    "                                'strategy'], axis=1).to_csv('coded_dataset_hierarchy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b53723a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Topic'] = doc_topic_labels\n",
    "for topic_num in range(7):\n",
    "    original_data_with_topics[f'Topic_{topic_num}'] = 0\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    topic = row['Topic']\n",
    "    original_data_with_topics.at[index, f'Topic_{topic}'] = 1\n",
    "\n",
    "original_data_with_topics.drop(['information', 'grounds', 'claim',\n",
    "                                'organization', 'query', 'social',\n",
    "                                'strategy'], axis=1).to_csv('coded_dataset_hierarchy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c868d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef1da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
